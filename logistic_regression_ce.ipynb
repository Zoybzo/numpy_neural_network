{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression With CrossEntropy Loss\n",
    "使用 Numpy 实现交叉熵损失的逻辑回归，包括前向传播、反向传播、损失计算、训练过程"
   ],
   "id": "3e87df7d7f5b7ebf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T01:15:41.754247Z",
     "start_time": "2025-02-27T01:15:38.679532Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install numpy",
   "id": "c5f779a0f180b68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\r\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\r\n",
      "Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: numpy\r\n",
      "Successfully installed numpy-2.0.2\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T01:15:44.614973Z",
     "start_time": "2025-02-27T01:15:43.915575Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "e4c4061ba9eb4ba8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cross Entropy Loss\n",
    "二分类的交叉熵损失\n",
    "$$\n",
    "\\mathcal{L} = -(y_{true}\\log(y_{pred}) + (1-y_{true})\\log(1-y_{pred}))\n",
    "$$\n",
    "交叉熵损失的导数\n",
    "$$\n",
    "\\mathcal{L}^\\prime = -(\\frac{y_{true}}{y_{pred}} - \\frac{1-y_{true}}{1-y_{pred}}) = \\frac{y_{pred} - y_{true}}{y_{pred}(1 - y_{pred})}\n",
    "$$\n",
    "在二分类任务中不需要通过函数来定义交叉熵损失的导数，因为sigmoid会和导数的分母抵消。"
   ],
   "id": "e2217c388be448f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    # 防止 log(0) 出现\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # clip的作用：限制 y_pred 的范围\n",
    "    # loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss"
   ],
   "id": "910602c132935dac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sigmoid",
   "id": "9f72ba02af57cf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(fx):\n",
    "    return fx * (1 - fx)"
   ],
   "id": "e4fb57d03938e32a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LogisticRegression",
   "id": "2bad1873b21c13ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, in_dim, hidden_dim, learning_rate=0.01):\n",
    "        self.in_dim = in_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w1 = np.random.randn(in_dim, hidden_dim)\n",
    "        self.b1 = np.random.randn(1, hidden_dim)\n",
    "        self.w2 = np.random.randn(hidden_dim, 1)\n",
    "        self.b2 = np.random.randn(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [bs, in_dim]\n",
    "        self.z1 = np.dot(x, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1) # [bs, hidden_dim]\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2) # [bs, 1]\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # layer2\n",
    "        layer2_error = y_pred - y_true # [bs, 1] # 对 z2 求导\n",
    "        dw2 = np.dot(self.a1.T, layer2_error) # [hidden_dim, 1]\n",
    "        db2 = np.sum(layer2_error, axis=0, keepdims=True) # [1, 1]\n",
    "        # layer1\n",
    "        layer1_error = np.dot(layer2_error, self.w2.T) * sigmoid_derivative(self.z1) # [bs, hidden_dim] # 对 z1 求导：先对a1求导，再对 z1 求导\n",
    "        dw1 = np.dot(x.T, layer1_error) # [in_dim, hidden_dim]\n",
    "        db1 = np.sum(layer1_error, axis=0, keepdims=True) # [1, hidden_dim]\n",
    "        # update\n",
    "        self.w2 -= self.learning_rate * dw2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.w1 -= self.learning_rate * dw1\n",
    "        self.b1 -= self.learning_rate * db1"
   ],
   "id": "87dd0f1cb2221041"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
