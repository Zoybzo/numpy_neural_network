{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T06:05:29.363056Z",
     "start_time": "2025-02-24T06:05:28.501070Z"
    }
   },
   "source": "!pip install numpy",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/mkpk/miniconda3/lib/python3.10/site-packages (1.26.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T06:07:09.511169Z",
     "start_time": "2025-02-24T06:07:09.454756Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "1446407f04396ff1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Numpy Linear Regression\n",
    "使用 Numpy 实现线性回归，包括前向传播、反向传播、损失计算、训练过程"
   ],
   "id": "a7185d589aef171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sigmoid 激活函数及其导数\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-1}}\n",
    "$$\n",
    "$$\n",
    "sigmoid(x)^\\prime = sigmoid(x) * (1 - sigmoid(x))\n",
    "$$"
   ],
   "id": "7e59c5eb34f9f1a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:49:45.614328Z",
     "start_time": "2025-02-24T08:49:45.611526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(fx):\n",
    "    return fx * (1 - fx)"
   ],
   "id": "21ff57e684a3da60",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MSE损失函数及其导数\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i -\\hat y_i)^2\n",
    "$$\n",
    "$$\n",
    "MSE^\\prime = 2 * \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat y_i)\n",
    "$$"
   ],
   "id": "98bdc38beeb19b5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:50:07.790724Z",
     "start_time": "2025-02-24T08:50:07.786163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred).mean()"
   ],
   "id": "3c984c36cb86d50d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LinearRegression",
   "id": "30a364b70e888f02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:48:45.495995Z",
     "start_time": "2025-02-24T08:48:45.481021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 0.01):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: [bs, input_size]\n",
    "        # layer1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        # [bs, hidden_size]\n",
    "        # layer2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        # [bs, output_size]\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y_true):\n",
    "        error_layer2 = mse_loss_derivative(y_true=y_true, y_pred=self.a2) * sigmoid_derivative(self.z2)\n",
    "        # [bs, output_size]\n",
    "        dw2 = np.dot(self.a1.T, error_layer2)\n",
    "        # [hidden_size, output_size]\n",
    "        db2 = np.sum(error_layer2, axis=0, keepdims=True)\n",
    "        # [1, output_size]\n",
    "        error_layer1 = np.dot(error_layer2, self.W2.T) * sigmoid_derivative(self.z1)\n",
    "        # [bs, hidden_size]\n",
    "        dw1 = np.dot(X.T, error_layer1)\n",
    "        # [input_size, hidden_size]\n",
    "        db1 = np.sum(error_layer1, axis=0, keepdims=True)\n",
    "        # [1, hidden_size]\n",
    "        # update\n",
    "        self.W1 -= self.learning_rate * dw1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dw2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        output = self.forward(X)\n",
    "        loss = mse_loss(y, output)\n",
    "        self.backward(X, y_true=y)\n",
    "        return loss\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.calculate_loss(X, y)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")"
   ],
   "id": "a81a1c9e5a1c99fb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T08:52:15.015673Z",
     "start_time": "2025-02-24T08:52:14.976592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例数据\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 输入\n",
    "y = np.array([[0], [1], [1], [0]])  # 二分类目标\n",
    "\n",
    "# 创建神经网络\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "nn = LinearRegression(input_size, hidden_size, output_size)\n",
    "\n",
    "# 训练网络\n",
    "nn.train(X, y, epochs=1000)\n",
    "\n",
    "# 测试\n",
    "print(\"Testing the network:\")\n",
    "for i in range(len(X)):\n",
    "    output = nn.forward(X[i:i+1])\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {output[0][0]:.4f}\")"
   ],
   "id": "551ad5a8a9c86c00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.2571\n",
      "Epoch 100/1000, Loss: 0.2755\n",
      "Epoch 200/1000, Loss: 0.2961\n",
      "Epoch 300/1000, Loss: 0.2970\n",
      "Epoch 400/1000, Loss: nan\n",
      "Epoch 500/1000, Loss: nan\n",
      "Epoch 600/1000, Loss: nan\n",
      "Epoch 700/1000, Loss: nan\n",
      "Epoch 800/1000, Loss: nan\n",
      "Epoch 900/1000, Loss: nan\n",
      "Testing the network:\n",
      "Input: [0 0], Predicted Output: nan\n",
      "Input: [0 1], Predicted Output: nan\n",
      "Input: [1 0], Predicted Output: nan\n",
      "Input: [1 1], Predicted Output: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/785dy0q51jz29kptswspwjpc0000gn/T/ipykernel_72586/4282551863.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/var/folders/75/785dy0q51jz29kptswspwjpc0000gn/T/ipykernel_72586/4282551863.py:5: RuntimeWarning: overflow encountered in multiply\n",
      "  return fx * (1 - fx)\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
